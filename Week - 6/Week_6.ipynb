{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"19nCi6_Ouy9vGy1RgLNLLfqqHJflTDkhI","authorship_tag":"ABX9TyPqBmBNvCWunPQ+wHGnDjk9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Constituency parsing and Probabilistic parsing\n","\n","Constituency parsing :\n","\n","   It is a type of syntactic parsing that aims to identify the constituents, or subparts, of a sentence and the relationships between them.\n","\n","Probabilistic parsing :  "],"metadata":{"id":"nXIV0umteG_F"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RdIoPtPhd4Ke","executionInfo":{"status":"ok","timestamp":1677680957318,"user_tz":-330,"elapsed":698,"user":{"displayName":"20R21A6632 MADADAPU HEMANTH SAI","userId":"17549942894635473307"}},"outputId":"e38cf1a2-2832-4f82-c5fb-d66511f154fc"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading collection 'popular'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Package cmudict is already up-to-date!\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Package gazetteers is already up-to-date!\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Package genesis is already up-to-date!\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Package gutenberg is already up-to-date!\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Package inaugural is already up-to-date!\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package movie_reviews is already up-to-date!\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Package names is already up-to-date!\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Package shakespeare is already up-to-date!\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Package stopwords is already up-to-date!\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Package treebank is already up-to-date!\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package twitter_samples is already up-to-date!\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Package omw is already up-to-date!\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    |   Package omw-1.4 is already up-to-date!\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Package wordnet is already up-to-date!\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    |   Package wordnet2021 is already up-to-date!\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    |   Package wordnet31 is already up-to-date!\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Package wordnet_ic is already up-to-date!\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Package words is already up-to-date!\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Package punkt is already up-to-date!\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package snowball_data is already up-to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection popular\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":18}],"source":["import nltk\n","nltk.download(\"popular\")"]},{"cell_type":"markdown","source":[" Dependency Parsing"],"metadata":{"id":"jK644cbSbuQK"}},{"cell_type":"code","source":["from nltk.parse.stanford import StanfordDependencyParser"],"metadata":{"id":"Pm4HeWOJgKCA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pj = \"/content/drive/MyDrive/NLP Lab/Week - 6/stanford-parser-4.2.0/stanford-parser-full-2020-11-17/stanford-parser-4.2.0-javadoc.jar\"\n","pmj = \"/content/drive/MyDrive/NLP Lab/Week - 6/stanford-parser-4.2.0/stanford-parser-full-2020-11-17/stanford-parser-4.2.0-models.jar\""],"metadata":{"id":"BdcvWVpVjXcq","executionInfo":{"status":"ok","timestamp":1677662774290,"user_tz":-330,"elapsed":699,"user":{"displayName":"20R21A6632 MADADAPU HEMANTH SAI","userId":"17549942894635473307"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["dep_parser = StanfordDependencyParser(path_to_jar=pj, path_to_models_jar=pmj)\n","result = dep_parser.raw_parse(\"I shot an elephant in my sleep\")\n","dependency = result.__next__()\n","list(dependency.triples())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bPT7mLGOgiYb","executionInfo":{"status":"ok","timestamp":1677662945674,"user_tz":-330,"elapsed":3958,"user":{"displayName":"20R21A6632 MADADAPU HEMANTH SAI","userId":"17549942894635473307"}},"outputId":"781f2b58-a9a1-4f83-fd14-b8dff9a47b80"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-25-257a1cd889dd>:1: DeprecationWarning: The StanfordDependencyParser will be deprecated\n","Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n","  dep_parser = StanfordDependencyParser(path_to_jar=pj, path_to_models_jar=pmj)\n"]},{"output_type":"execute_result","data":{"text/plain":["[(('shot', 'VBD'), 'nsubj', ('I', 'PRP')),\n"," (('shot', 'VBD'), 'obj', ('elephant', 'NN')),\n"," (('elephant', 'NN'), 'det', ('an', 'DT')),\n"," (('shot', 'VBD'), 'obl', ('sleep', 'NN')),\n"," (('sleep', 'NN'), 'case', ('in', 'IN')),\n"," (('sleep', 'NN'), 'nmod:poss', ('my', 'PRP$'))]"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["Reference : https://towardsdatascience.com/natural-language-processing-dependency-parsing-cf094bbbe3f7"],"metadata":{"id":"4pnyW5unbz-f"}}]}